{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4f79c0b",
   "metadata": {},
   "source": [
    "Apprendre spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97851447",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/22 04:38:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "\n",
    "# Reduce logging level\n",
    "logging.getLogger(\"pyspark\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
    "\n",
    "# Create Spark session with quieter configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set Spark's log level to ERROR (shows only errors)\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885f0e2c",
   "metadata": {},
   "source": [
    "Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "821cca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From a list of dictionaries\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "columns = [\"name\", \"age\"]\n",
    "df_manuel = spark.createDataFrame(data, columns)\n",
    "\n",
    "# From a CSV file\n",
    "df_csv = spark.read.csv(\"input/file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# From JSON\n",
    "df_json = spark.read.json(\"input/file.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2e4cd16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, age: bigint]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_manuel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce588172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, name: string, age: int, department: string, salary: int, hire_date: date]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1a74522",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01dccd2",
   "metadata": {},
   "source": [
    "Import de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddcd866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, count, max, min, year, current_date, datediff\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18f3e39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données originales :\n",
      "+---+--------------+---+-----------+------+----------+\n",
      "| id|          name|age| department|salary| hire_date|\n",
      "+---+--------------+---+-----------+------+----------+\n",
      "|  1| Alice Johnson| 28|Engineering| 75000|2020-03-15|\n",
      "|  2|     Bob Smith| 34|  Marketing| 65000|2019-07-22|\n",
      "|  3| Charlie Brown| 41|Engineering| 95000|2018-01-10|\n",
      "|  4|  Diana Prince| 29|         HR| 58000|2021-05-03|\n",
      "|  5| Edward Wilson| 52|    Finance| 82000|2017-11-28|\n",
      "|  6|   Fiona Davis| 26|  Marketing| 52000|2022-02-14|\n",
      "|  7| George Miller| 38|Engineering| 88000|2019-09-05|\n",
      "|  8|  Helen Garcia| 45|    Finance| 76000|2018-06-18|\n",
      "|  9|Ivan Rodriguez| 31|         HR| 61000|2020-12-01|\n",
      "| 10|Julia Anderson| 27|Engineering| 72000|2021-08-20|\n",
      "+---+--------------+---+-----------+------+----------+\n",
      "\n",
      "Schéma :\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n",
      "\n",
      "=== OPÉRATIONS DE BASE ===\n",
      "Nombre total d'employés :\n",
      "10\n",
      "\n",
      "Premières 5 lignes :\n",
      "+---+-------------+---+-----------+------+----------+\n",
      "| id|         name|age| department|salary| hire_date|\n",
      "+---+-------------+---+-----------+------+----------+\n",
      "|  1|Alice Johnson| 28|Engineering| 75000|2020-03-15|\n",
      "|  2|    Bob Smith| 34|  Marketing| 65000|2019-07-22|\n",
      "|  3|Charlie Brown| 41|Engineering| 95000|2018-01-10|\n",
      "|  4| Diana Prince| 29|         HR| 58000|2021-05-03|\n",
      "|  5|Edward Wilson| 52|    Finance| 82000|2017-11-28|\n",
      "+---+-------------+---+-----------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Colonnes spécifiques :\n",
      "+--------------+-----------+------+\n",
      "|          name| department|salary|\n",
      "+--------------+-----------+------+\n",
      "| Alice Johnson|Engineering| 75000|\n",
      "|     Bob Smith|  Marketing| 65000|\n",
      "| Charlie Brown|Engineering| 95000|\n",
      "|  Diana Prince|         HR| 58000|\n",
      "| Edward Wilson|    Finance| 82000|\n",
      "|   Fiona Davis|  Marketing| 52000|\n",
      "| George Miller|Engineering| 88000|\n",
      "|  Helen Garcia|    Finance| 76000|\n",
      "|Ivan Rodriguez|         HR| 61000|\n",
      "|Julia Anderson|Engineering| 72000|\n",
      "+--------------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialiser Spark\n",
    "\n",
    "print(\"Données originales :\")\n",
    "df.show()\n",
    "\n",
    "print(\"Schéma :\")\n",
    "df.printSchema()\n",
    "\n",
    "# 2. OPÉRATIONS DE BASE\n",
    "print(\"\\n=== OPÉRATIONS DE BASE ===\")\n",
    "\n",
    "print(\"Nombre total d'employés :\")\n",
    "print(df.count())\n",
    "\n",
    "print(\"\\nPremières 5 lignes :\")\n",
    "df.show(5)\n",
    "\n",
    "print(\"\\nColonnes spécifiques :\")\n",
    "df.select(\"name\", \"department\", \"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae28cd0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c291b845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FILTRAGE ===\n",
      "Employés avec salaire > 70000 :\n",
      "+---+--------------+---+-----------+------+----------+\n",
      "| id|          name|age| department|salary| hire_date|\n",
      "+---+--------------+---+-----------+------+----------+\n",
      "|  1| Alice Johnson| 28|Engineering| 75000|2020-03-15|\n",
      "|  3| Charlie Brown| 41|Engineering| 95000|2018-01-10|\n",
      "|  5| Edward Wilson| 52|    Finance| 82000|2017-11-28|\n",
      "|  7| George Miller| 38|Engineering| 88000|2019-09-05|\n",
      "|  8|  Helen Garcia| 45|    Finance| 76000|2018-06-18|\n",
      "| 10|Julia Anderson| 27|Engineering| 72000|2021-08-20|\n",
      "+---+--------------+---+-----------+------+----------+\n",
      "\n",
      "Employés du département Engineering :\n",
      "+---+--------------+---+-----------+------+----------+\n",
      "| id|          name|age| department|salary| hire_date|\n",
      "+---+--------------+---+-----------+------+----------+\n",
      "|  1| Alice Johnson| 28|Engineering| 75000|2020-03-15|\n",
      "|  3| Charlie Brown| 41|Engineering| 95000|2018-01-10|\n",
      "|  7| George Miller| 38|Engineering| 88000|2019-09-05|\n",
      "| 10|Julia Anderson| 27|Engineering| 72000|2021-08-20|\n",
      "+---+--------------+---+-----------+------+----------+\n",
      "\n",
      "Employés de moins de 30 ans :\n",
      "+---+--------------+---+-----------+------+----------+\n",
      "| id|          name|age| department|salary| hire_date|\n",
      "+---+--------------+---+-----------+------+----------+\n",
      "|  1| Alice Johnson| 28|Engineering| 75000|2020-03-15|\n",
      "|  4|  Diana Prince| 29|         HR| 58000|2021-05-03|\n",
      "|  6|   Fiona Davis| 26|  Marketing| 52000|2022-02-14|\n",
      "| 10|Julia Anderson| 27|Engineering| 72000|2021-08-20|\n",
      "+---+--------------+---+-----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. FILTRAGE\n",
    "print(\"\\n=== FILTRAGE ===\")\n",
    "\n",
    "print(\"Employés avec salaire > 70000 :\")\n",
    "df.filter(col(\"salary\") > 70000).show()\n",
    "\n",
    "print(\"Employés du département Engineering :\")\n",
    "df.filter(col(\"department\") == \"Engineering\").show()\n",
    "\n",
    "print(\"Employés de moins de 30 ans :\")\n",
    "df.filter(col(\"age\") < 30).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e70f27d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AGRÉGATIONS ===\n",
      "Statistiques générales des salaires :\n",
      "+-----------+-----------+-------------+\n",
      "|salaire_min|salaire_max|salaire_moyen|\n",
      "+-----------+-----------+-------------+\n",
      "|      52000|      95000|      72400.0|\n",
      "+-----------+-----------+-------------+\n",
      "\n",
      "Nombre d'employés par département :\n",
      "+-----------+-----+\n",
      "| department|count|\n",
      "+-----------+-----+\n",
      "|Engineering|    4|\n",
      "|         HR|    2|\n",
      "|    Finance|    2|\n",
      "|  Marketing|    2|\n",
      "+-----------+-----+\n",
      "\n",
      "Salaire moyen par département :\n",
      "+-----------+-------------+\n",
      "| department|salaire_moyen|\n",
      "+-----------+-------------+\n",
      "|Engineering|      82500.0|\n",
      "|    Finance|      79000.0|\n",
      "|         HR|      59500.0|\n",
      "|  Marketing|      58500.0|\n",
      "+-----------+-------------+\n",
      "\n",
      "Age moyen par département :\n",
      "+-----------+---------+\n",
      "| department|age_moyen|\n",
      "+-----------+---------+\n",
      "|Engineering|     33.5|\n",
      "|         HR|     30.0|\n",
      "|    Finance|     48.5|\n",
      "|  Marketing|     30.0|\n",
      "+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. AGRÉGATIONS\n",
    "print(\"\\n=== AGRÉGATIONS ===\")\n",
    "\n",
    "print(\"Statistiques générales des salaires :\")\n",
    "df.agg(\n",
    "    min(\"salary\").alias(\"salaire_min\"),\n",
    "    max(\"salary\").alias(\"salaire_max\"),\n",
    "    avg(\"salary\").alias(\"salaire_moyen\")\n",
    ").show()\n",
    "\n",
    "print(\"Nombre d'employés par département :\")\n",
    "df.groupBy(\"department\").count().orderBy(\"count\", ascending=False).show()\n",
    "\n",
    "print(\"Salaire moyen par département :\")\n",
    "df.groupBy(\"department\") \\\n",
    "  .agg(avg(\"salary\").alias(\"salaire_moyen\")) \\\n",
    "  .orderBy(\"salaire_moyen\", ascending=False) \\\n",
    "  .show()\n",
    "\n",
    "print(\"Age moyen par département :\")\n",
    "df.groupBy(\"department\") \\\n",
    "  .agg(avg(\"age\").alias(\"age_moyen\")) \\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc03732a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRANSFORMATIONS AVANCÉES ===\n",
      "Avec catégorie d'âge :\n",
      "+--------------+---+------------+\n",
      "|          name|age|age_category|\n",
      "+--------------+---+------------+\n",
      "| Alice Johnson| 28|       Jeune|\n",
      "|     Bob Smith| 34|       Moyen|\n",
      "| Charlie Brown| 41|      Senior|\n",
      "|  Diana Prince| 29|       Jeune|\n",
      "| Edward Wilson| 52|      Senior|\n",
      "|   Fiona Davis| 26|       Jeune|\n",
      "| George Miller| 38|       Moyen|\n",
      "|  Helen Garcia| 45|      Senior|\n",
      "|Ivan Rodriguez| 31|       Moyen|\n",
      "|Julia Anderson| 27|       Jeune|\n",
      "+--------------+---+------------+\n",
      "\n",
      "Avec ancienneté :\n",
      "+--------------+----------+------------+\n",
      "|          name| hire_date|tenure_years|\n",
      "+--------------+----------+------------+\n",
      "| Charlie Brown|2018-01-10|           7|\n",
      "| Edward Wilson|2017-11-28|           7|\n",
      "|  Helen Garcia|2018-06-18|           7|\n",
      "|     Bob Smith|2019-07-22|           6|\n",
      "| George Miller|2019-09-05|           6|\n",
      "| Alice Johnson|2020-03-15|           5|\n",
      "|  Diana Prince|2021-05-03|           4|\n",
      "|Ivan Rodriguez|2020-12-01|           4|\n",
      "|Julia Anderson|2021-08-20|           4|\n",
      "|   Fiona Davis|2022-02-14|           3|\n",
      "+--------------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. TRANSFORMATIONS AVANCÉES\n",
    "print(\"\\n=== TRANSFORMATIONS AVANCÉES ===\")\n",
    "\n",
    "# Ajouter une colonne catégorie d'âge\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df_with_category = df.withColumn(\n",
    "    \"age_category\",\n",
    "    when(col(\"age\") < 30, \"Jeune\")\n",
    "    .when(col(\"age\") < 40, \"Moyen\")\n",
    "    .otherwise(\"Senior\")\n",
    ")\n",
    "\n",
    "print(\"Avec catégorie d'âge :\")\n",
    "df_with_category.select(\"name\", \"age\", \"age_category\").show()\n",
    "\n",
    "# Calculer l'ancienneté - VERSION SIMPLIFIÉE\n",
    "df_with_tenure = df.withColumn(\n",
    "    \"tenure_years\",\n",
    "    (datediff(current_date(), col(\"hire_date\")) / 365).cast(\"int\")\n",
    ")\n",
    "\n",
    "print(\"Avec ancienneté :\")\n",
    "df_with_tenure.select(\"name\", \"hire_date\", \"tenure_years\") \\\n",
    "               .orderBy(\"tenure_years\", ascending=False) \\\n",
    "               .show()  # Pas de point ici !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0d51c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ANALYSES SPÉCIFIQUES ===\n",
      "Top 3 des salaires les plus élevés :\n",
      "+-------------+-----------+------+\n",
      "|         name| department|salary|\n",
      "+-------------+-----------+------+\n",
      "|Charlie Brown|Engineering| 95000|\n",
      "|George Miller|Engineering| 88000|\n",
      "|Edward Wilson|    Finance| 82000|\n",
      "+-------------+-----------+------+\n",
      "\n",
      "Employés embauchés en 2020 ou après :\n",
      "+--------------+-----------+----------+\n",
      "|          name| department| hire_date|\n",
      "+--------------+-----------+----------+\n",
      "| Alice Johnson|Engineering|2020-03-15|\n",
      "|Ivan Rodriguez|         HR|2020-12-01|\n",
      "|  Diana Prince|         HR|2021-05-03|\n",
      "|Julia Anderson|Engineering|2021-08-20|\n",
      "|   Fiona Davis|  Marketing|2022-02-14|\n",
      "+--------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. ANALYSES SPÉCIFIQUES\n",
    "print(\"\\n=== ANALYSES SPÉCIFIQUES ===\")\n",
    "\n",
    "print(\"Top 3 des salaires les plus élevés :\")\n",
    "df.select(\"name\", \"department\", \"salary\") \\\n",
    "  .orderBy(\"salary\", ascending=False) \\\n",
    "  .limit(3) \\\n",
    "  .show()\n",
    "\n",
    "print(\"Employés embauchés en 2020 ou après :\")\n",
    "df.filter(year(\"hire_date\") >= 2020) \\\n",
    "  .select(\"name\", \"department\", \"hire_date\") \\\n",
    "  .orderBy(\"hire_date\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75c80a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RENOMMAGE DES COLONNES ===\n",
      "DataFrame avec noms français :\n",
      "+---+--------------+---+-----------+-------+-------------+\n",
      "| id|           nom|age|departement|salaire|date_embauche|\n",
      "+---+--------------+---+-----------+-------+-------------+\n",
      "|  1| Alice Johnson| 28|Engineering|  75000|   2020-03-15|\n",
      "|  2|     Bob Smith| 34|  Marketing|  65000|   2019-07-22|\n",
      "|  3| Charlie Brown| 41|Engineering|  95000|   2018-01-10|\n",
      "|  4|  Diana Prince| 29|         HR|  58000|   2021-05-03|\n",
      "|  5| Edward Wilson| 52|    Finance|  82000|   2017-11-28|\n",
      "|  6|   Fiona Davis| 26|  Marketing|  52000|   2022-02-14|\n",
      "|  7| George Miller| 38|Engineering|  88000|   2019-09-05|\n",
      "|  8|  Helen Garcia| 45|    Finance|  76000|   2018-06-18|\n",
      "|  9|Ivan Rodriguez| 31|         HR|  61000|   2020-12-01|\n",
      "| 10|Julia Anderson| 27|Engineering|  72000|   2021-08-20|\n",
      "+---+--------------+---+-----------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7. RENOMMER LES COLONNES\n",
    "print(\"\\n=== RENOMMAGE DES COLONNES ===\")\n",
    "\n",
    "df_french = df.withColumnRenamed(\"name\", \"nom\") \\\n",
    "              .withColumnRenamed(\"age\", \"age\") \\\n",
    "              .withColumnRenamed(\"department\", \"departement\") \\\n",
    "              .withColumnRenamed(\"salary\", \"salaire\") \\\n",
    "              .withColumnRenamed(\"hire_date\", \"date_embauche\")\n",
    "\n",
    "print(\"DataFrame avec noms français :\")\n",
    "df_french.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba66a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspaces/Daily/.venv/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/Daily/.venv/lib/python3.11/site-packages/py4j/clientserver.py\", line 535, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/socket.py\", line 718, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données originales :\n",
      "+---+--------------+---+-----------+------+----------+\n",
      "| id|          name|age| department|salary| hire_date|\n",
      "+---+--------------+---+-----------+------+----------+\n",
      "|  1| Alice Johnson| 28|Engineering| 75000|2020-03-15|\n",
      "|  2|     Bob Smith| 34|  Marketing| 65000|2019-07-22|\n",
      "|  3| Charlie Brown| 41|Engineering| 95000|2018-01-10|\n",
      "|  4|  Diana Prince| 29|         HR| 58000|2021-05-03|\n",
      "|  5| Edward Wilson| 52|    Finance| 82000|2017-11-28|\n",
      "|  6|   Fiona Davis| 26|  Marketing| 52000|2022-02-14|\n",
      "|  7| George Miller| 38|Engineering| 88000|2019-09-05|\n",
      "|  8|  Helen Garcia| 45|    Finance| 76000|2018-06-18|\n",
      "|  9|Ivan Rodriguez| 31|         HR| 61000|2020-12-01|\n",
      "| 10|Julia Anderson| 27|Engineering| 72000|2021-08-20|\n",
      "+---+--------------+---+-----------+------+----------+\n",
      "\n",
      "Schéma :\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n",
      "\n",
      "=== OPÉRATIONS DE BASE ===\n",
      "Nombre total d'employés :\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== OPÉRATIONS DE BASE ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNombre total d\u001b[39m\u001b[33m'\u001b[39m\u001b[33memployés :\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPremières 5 lignes :\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m df.show(\u001b[32m5\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Daily/.venv/lib/python3.11/site-packages/pyspark/sql/classic/dataframe.py:439\u001b[39m, in \u001b[36mDataFrame.count\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Daily/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1361\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1354\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1361\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m return_value = get_return_value(\n\u001b[32m   1363\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Daily/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1036\u001b[39m connection = \u001b[38;5;28mself\u001b[39m._get_connection()\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[32m   1040\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m._create_connection_guard(connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Daily/.venv/lib/python3.11/site-packages/py4j/clientserver.py:535\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    534\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m         answer = smart_decode(\u001b[38;5;28mself\u001b[39m.stream.readline()[:-\u001b[32m1\u001b[39m])\n\u001b[32m    536\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m\"\u001b[39m.format(answer))\n\u001b[32m    537\u001b[39m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[32m    538\u001b[39m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/socket.py:718\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    720\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 8. STATISTIQUES PAR DÉPARTEMENT\n",
    "print(\"\\n=== STATISTIQUES DÉTAILLÉES PAR DÉPARTEMENT ===\")\n",
    "\n",
    "dept_stats = df.groupBy(\"department\").agg(\n",
    "    count(\"*\").alias(\"nombre_employes\"),\n",
    "    avg(\"salary\").alias(\"salaire_moyen\"),\n",
    "    min(\"salary\").alias(\"salaire_min\"),\n",
    "    max(\"salary\").alias(\"salaire_max\"),\n",
    "    avg(\"age\").alias(\"age_moyen\")\n",
    ")\n",
    "\n",
    "dept_stats.show()\n",
    "\n",
    "# Fermer Spark\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abd94be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---+-----------+------+----------+\n",
      "| id|          name|age| department|salary| hire_date|\n",
      "+---+--------------+---+-----------+------+----------+\n",
      "|  1| Alice Johnson| 28|Engineering| 75000|2020-03-15|\n",
      "|  2|     Bob Smith| 34|  Marketing| 65000|2019-07-22|\n",
      "|  3| Charlie Brown| 41|Engineering| 95000|2018-01-10|\n",
      "|  4|  Diana Prince| 29|         HR| 58000|2021-05-03|\n",
      "|  5| Edward Wilson| 52|    Finance| 82000|2017-11-28|\n",
      "|  6|   Fiona Davis| 26|  Marketing| 52000|2022-02-14|\n",
      "|  7| George Miller| 38|Engineering| 88000|2019-09-05|\n",
      "|  8|  Helen Garcia| 45|    Finance| 76000|2018-06-18|\n",
      "|  9|Ivan Rodriguez| 31|         HR| 61000|2020-12-01|\n",
      "| 10|Julia Anderson| 27|Engineering| 72000|2021-08-20|\n",
      "+---+--------------+---+-----------+------+----------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n",
      "+--------------+---+\n",
      "|          name|age|\n",
      "+--------------+---+\n",
      "| Alice Johnson| 28|\n",
      "|     Bob Smith| 34|\n",
      "| Charlie Brown| 41|\n",
      "|  Diana Prince| 29|\n",
      "| Edward Wilson| 52|\n",
      "|   Fiona Davis| 26|\n",
      "| George Miller| 38|\n",
      "|  Helen Garcia| 45|\n",
      "|Ivan Rodriguez| 31|\n",
      "|Julia Anderson| 27|\n",
      "+--------------+---+\n",
      "\n",
      "+---+--------------+---+-----------+------+----------+\n",
      "| id|          name|age| department|salary| hire_date|\n",
      "+---+--------------+---+-----------+------+----------+\n",
      "|  1| Alice Johnson| 28|Engineering| 75000|2020-03-15|\n",
      "|  2|     Bob Smith| 34|  Marketing| 65000|2019-07-22|\n",
      "|  3| Charlie Brown| 41|Engineering| 95000|2018-01-10|\n",
      "|  4|  Diana Prince| 29|         HR| 58000|2021-05-03|\n",
      "|  5| Edward Wilson| 52|    Finance| 82000|2017-11-28|\n",
      "|  6|   Fiona Davis| 26|  Marketing| 52000|2022-02-14|\n",
      "|  7| George Miller| 38|Engineering| 88000|2019-09-05|\n",
      "|  8|  Helen Garcia| 45|    Finance| 76000|2018-06-18|\n",
      "|  9|Ivan Rodriguez| 31|         HR| 61000|2020-12-01|\n",
      "| 10|Julia Anderson| 27|Engineering| 72000|2021-08-20|\n",
      "+---+--------------+---+-----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the data\n",
    "df.show()\n",
    "\n",
    "# Display schema\n",
    "df.printSchema()\n",
    "\n",
    "# Select columns\n",
    "df.select(\"name\", \"age\").show()\n",
    "\n",
    "# Filter rows\n",
    "df.filter(df.age > 25).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b414e164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2025-09-22 04:30:55.498\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `category` cannot be resolved. Did you mean one of the following? [`age`, `name`, `salary`, `id`, `department`]. SQLSTATE: 42703\", \"context\": {\"file\": \"java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\", \"line\": \"\", \"fragment\": \"col\", \"errorClass\": \"UNRESOLVED_COLUMN.WITH_SUGGESTION\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o140.count.\\n: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `category` cannot be resolved. Did you mean one of the following? [`age`, `name`, `salary`, `id`, `department`]. SQLSTATE: 42703;\\n'Aggregate ['category], ['category, count(1) AS count#194L]\\n+- Relation [id#56,name#57,age#58,department#59,salary#60,hire_date#61] csv\\n\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\\n\\tat org.apache.spark.sql.classic.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:84)\\n\\tat org.apache.spark.sql.classic.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:54)\\n\\tat org.apache.spark.sql.RelationalGroupedDataset.count(RelationalGroupedDataset.scala:166)\\n\\tat org.apache.spark.sql.classic.RelationalGroupedDataset.count(RelationalGroupedDataset.scala:150)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:1583)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 22 more\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/workspaces/Daily/.venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/workspaces/Daily/.venv/lib/python3.11/site-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `category` cannot be resolved. Did you mean one of the following? [`age`, `name`, `salary`, `id`, `department`]. SQLSTATE: 42703;\n'Aggregate ['category], ['category, count(1) AS count#194L]\n+- Relation [id#56,name#57,age#58,department#59,salary#60,hire_date#61] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m df_with_category = df.withColumn(\u001b[33m\"\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m      4\u001b[39m     when(col(\u001b[33m\"\u001b[39m\u001b[33mage\u001b[39m\u001b[33m\"\u001b[39m) < \u001b[32m30\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33myoung\u001b[39m\u001b[33m\"\u001b[39m).otherwise(\u001b[33m\"\u001b[39m\u001b[33madult\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Group by and aggregate\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcategory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Daily/.venv/lib/python3.11/site-packages/pyspark/sql/group.py:37\u001b[39m, in \u001b[36mdfapi.<locals>._api\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_api\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mGroupedData\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mDataFrame\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     36\u001b[39m     name = f.\u001b[34m__name__\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     jdf = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jgd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m.session)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Daily/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Daily/.venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `category` cannot be resolved. Did you mean one of the following? [`age`, `name`, `salary`, `id`, `department`]. SQLSTATE: 42703;\n'Aggregate ['category], ['category, count(1) AS count#194L]\n+- Relation [id#56,name#57,age#58,department#59,salary#60,hire_date#61] csv\n"
     ]
    }
   ],
   "source": [
    "# Add a new column\n",
    "from pyspark.sql.functions import col, when\n",
    "df_with_category = df.withColumn(\"category\", \n",
    "    when(col(\"age\") < 30, \"young\").otherwise(\"adult\"))\n",
    "\n",
    "# Group by and aggregate\n",
    "df.groupBy(\"category\").count().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daily (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
