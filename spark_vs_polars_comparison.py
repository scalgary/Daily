

import time
import polars as pl
import psutil
import multiprocessing
from pyspark.sql import SparkSession
from pathlib import Path
from pyspark.sql import functions as F
import pyarrow 

def monitor_memory():
    """Affiche l'utilisation mÃ©moire"""
    mem = psutil.virtual_memory()
    return f"RAM: {mem.percent}% ({mem.used/1024**3:.1f}GB utilisÃ©s)"

# ============================================
# CONFIGURATION DES RESSOURCES
# ============================================
num_cores = multiprocessing.cpu_count()
total_memory_gb = psutil.virtual_memory().total / (1024**3)
spark_memory = int(total_memory_gb * 1)  # 70% de la RAM totale

print("="*60)
print("CONFIGURATION DES RESSOURCES")
print("="*60)
print(f"CÅ“urs disponibles: {num_cores}")
print(f"RAM totale: {total_memory_gb:.1f} GB")
print(f"RAM pour Spark: {spark_memory} GB")
print(f"{monitor_memory()}")

# ============================================
# 1. TEST AVEC POLARS
# ============================================
print("\n" + "="*60)
print("TEST POLARS - Lecture optimisÃ©e")
print("="*60)

start_total_polars = time.time()

# 1. LECTURE LAZY
print("\nâ–º Lecture LAZY du fichier...")
start = time.time()
df_polars = pl.scan_csv("data_medium.csv")
print(f" âœ“ Scan lazy: {time.time()-start:.3f}s")
print(f" {monitor_memory()}")

# Afficher le schÃ©ma (CORRECTION ICI)
print("\nâ–º SchÃ©ma des donnÃ©es:")
schema = df_polars.collect_schema()
column_names = schema.names()  # Utiliser .names() au lieu de .columns
print(f" Colonnes: {column_names}")
print(f" Types: {schema}")

# 2. OPÃ‰RATION DE TEST (exemple: groupby + aggregation)
print("\nâ–º ExÃ©cution d'une requÃªte (groupby + count)...")
start = time.time()
result_polars = (
    df_polars
    .group_by(column_names[0])
    .agg(pl.len())
    .sort(column_names[0])  # â† Ajout du tri
    .collect()
)
time_query_polars = time.time() - start
print(result_polars.head(10))  # 10 premiÃ¨res lignes
print(f" âœ“ RequÃªte exÃ©cutÃ©e: {time_query_polars:.3f}s")
print(f" {monitor_memory()}")
print(f" RÃ©sultat: {len(result_polars)} groupes")

time_total_polars = time.time() - start_total_polars
print(f"\nâ±ï¸  TEMPS TOTAL POLARS: {time_total_polars:.3f}s")

# ============================================
# 2. TEST AVEC SPARK
# ============================================
print("\n" + "="*60)
print("TEST SPARK - Configuration maximale")
print("="*60)

start_total_spark = time.time()

# Configuration Spark optimisÃ©e
print("\nâ–º Initialisation de Spark...")
start = time.time()
spark = SparkSession.builder \
    .appName("SparkBenchmark") \
    .master(f"local[{num_cores}]") \
    .config("spark.driver.memory", f"{spark_memory}g") \
    .config("spark.executor.memory", f"{spark_memory}g") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .config("spark.default.parallelism", num_cores * 2) \
    .config("spark.sql.shuffle.partitions", num_cores * 2) \
    .getOrCreate()
print(f" âœ“ Spark initialisÃ©: {time.time()-start:.3f}s")
print(f" {monitor_memory()}")

# 1. LECTURE LAZY
print("\nâ–º Lecture du fichier (lazy en Spark aussi)...")
start = time.time()
df_spark = spark.read.option("header", "true").option("inferSchema", "true").csv("data_medium.csv")
print(f" âœ“ DÃ©finition du DataFrame: {time.time()-start:.3f}s")
print(f" {monitor_memory()}")

# Afficher le schÃ©ma
print("\nâ–º SchÃ©ma des donnÃ©es:")
print(f" Colonnes: {df_spark.columns}")
df_spark.printSchema()

# 2. OPÃ‰RATION DE TEST (mÃªme opÃ©ration que Polars)
print("\nâ–º ExÃ©cution d'une requÃªte (groupby + count)...")
start = time.time()
#result_spark = df_spark.groupBy(df_spark.columns[0]).count().collect()
result_spark = (
    df_spark
    .groupBy(df_spark.columns[0])
    .agg(F.count("*").alias("len"))
    .orderBy(df_spark.columns[0])  # â† Ajout du tri
    .collect()
)
time_query_spark = time.time() - start
print(result_spark[:10])  # Affiche les 10 premiÃ¨res lignes

print(f" âœ“ RequÃªte exÃ©cutÃ©e: {time_query_spark:.3f}s")
print(f" {monitor_memory()}")
print(f" RÃ©sultat: {len(result_spark)} groupes")

time_total_spark = time.time() - start_total_spark
print(f"\nâ±ï¸  TEMPS TOTAL SPARK: {time_total_spark:.3f}s")

# ============================================
# 3. COMPARAISON FINALE
# ============================================
print("\n" + "="*60)
print("RÃ‰SULTATS DU BENCHMARK")
print("="*60)
print(f"\nPolars:")
print(f"  - Temps de requÃªte: {time_query_polars:.3f}s")
print(f"  - Temps total: {time_total_polars:.3f}s")
print(f"\nSpark:")
print(f"  - Temps de requÃªte: {time_query_spark:.3f}s")
print(f"  - Temps total: {time_total_spark:.3f}s")

# Calcul du speedup
speedup = time_query_spark / time_query_polars
print(f"\nðŸ† Polars est {speedup:.2f}x {'plus rapide' if speedup > 1 else 'plus lent'} que Spark")
print(f"   (pour cette requÃªte spÃ©cifique)")
result_spark_df = spark.createDataFrame(result_spark)
# Afficher les rÃ©sultats cÃ´te Ã  cÃ´te
print("\nâ–º POLARS (10 premiÃ¨res lignes):")
print(result_polars.head(10))

print("\nâ–º SPARK (10 premiÃ¨res lignes):")
result_spark_df.show(10)

# VÃ©rification : nombre de groupes
print("\nâ–º VÃ©rification basique:")
print(f"  Nombre de groupes Polars: {len(result_polars)}")
print(f"  Nombre de groupes Spark: {len(result_spark)}")

if len(result_polars) == len(result_spark):
    print("  âœ“ MÃªme nombre de groupes")
else:
    print("  âœ— ATTENTION: Nombre de groupes diffÃ©rent!")

# Comparaison dÃ©taillÃ©e (conversion en Pandas pour faciliter)
print("\nâ–º Comparaison dÃ©taillÃ©e des valeurs...")
result_polars_pd = result_polars.sort(column_names[0]).to_pandas()
result_spark_pd = result_spark_df.toPandas().sort_values(by=result_spark_df.columns[0]).reset_index(drop=True)

# Renommer les colonnes de Spark pour matcher Polars si nÃ©cessaire
result_spark_pd.columns = result_polars_pd.columns

# Comparer les DataFrames
if result_polars_pd.equals(result_spark_pd):
    print("  âœ“ Les rÃ©sultats sont IDENTIQUES")
else:
    print("  âš  Les rÃ©sultats diffÃ¨rent lÃ©gÃ¨rement")
    
    # Trouver les diffÃ©rences
    comparison = result_polars_pd.compare(result_spark_pd)
    if not comparison.empty:
        print("\n  DiffÃ©rences trouvÃ©es:")
        print(comparison.head(20))
    
    # VÃ©rifier les sommes totales
    sum_polars = result_polars_pd['len'].sum()
    sum_spark = result_spark_pd['len'].sum()
    print(f"\n  Somme totale Polars: {sum_polars}")
    print(f"  Somme totale Spark: {sum_spark}")
    
    if sum_polars == sum_spark:
        print("  âœ“ Les sommes totales correspondent (diffÃ©rences probablement dues au tri)")
# ============================================
# 4. COMPARAISON DÃ‰TAILLÃ‰E DES RÃ‰SULTATS
# ============================================
print("\n" + "="*60)
print("COMPARAISON DÃ‰TAILLÃ‰E DES RÃ‰SULTATS")
print("="*60)

# Convertir Spark en DataFrame
result_spark_df = spark.createDataFrame(result_spark)

# 1. APERÃ‡U VISUEL CÃ”TE Ã€ CÃ”TE
print("\nâ–º APERÃ‡U DES RÃ‰SULTATS:")
print("\nPOLARS (10 premiÃ¨res lignes):")
print(result_polars.head(10))

print("\nSPARK (10 premiÃ¨res lignes):")
result_spark_df.show(10, truncate=False)

# 2. STATISTIQUES DE BASE
print("\nâ–º STATISTIQUES:")
print(f"Nombre de groupes Polars: {len(result_polars)}")
print(f"Nombre de groupes Spark:  {len(result_spark)}")

sum_polars = result_polars.select(pl.col("len").sum()).item()
sum_spark = sum(row['len'] for row in result_spark)
print(f"\nSomme totale des counts:")
print(f"  Polars: {sum_polars:,}")
print(f"  Spark:  {sum_spark:,}")
print(f"  DiffÃ©rence: {abs(sum_polars - sum_spark):,}")

# 3. CONVERSION EN PANDAS POUR COMPARAISON DÃ‰TAILLÃ‰E
print("\nâ–º COMPARAISON DÃ‰TAILLÃ‰E (via Pandas)...")
result_polars_pd = result_polars.to_pandas().sort_values(by=column_names[0]).reset_index(drop=True)
result_spark_pd = result_spark_df.toPandas().sort_values(by=result_spark_df.columns[0]).reset_index(drop=True)
# Trier les deux DataFrames de la mÃªme maniÃ¨re
result_polars_pd = (
    result_polars
    .to_pandas()
    .sort_values(by=column_names[0])
    .reset_index(drop=True)
)

result_spark_pd = (
    result_spark_df
    .toPandas()
    .sort_values(by=result_spark_df.columns[0])
    .reset_index(drop=True)
)
# S'assurer que les colonnes ont les mÃªmes noms
result_spark_pd.columns = result_polars_pd.columns

# VÃ‰RIFICATION SUPPLÃ‰MENTAIRE : S'assurer que les types sont identiques
print(f"\nTypes Polars: {result_polars_pd.dtypes.to_dict()}")
print(f"Types Spark:  {result_spark_pd.dtypes.to_dict()}")

# Convertir les types si nÃ©cessaire pour une comparaison stricte
# (parfois Spark peut retourner int64 alors que Polars retourne int32, ou vice versa)
for col in result_polars_pd.columns:
    if col == 'len':
        # S'assurer que les counts sont du mÃªme type
        result_polars_pd[col] = result_polars_pd[col].astype('int64')
        result_spark_pd[col] = result_spark_pd[col].astype('int64')
    else:
        # Pour la colonne de groupement, convertir en string si nÃ©cessaire
        if result_polars_pd[col].dtype != result_spark_pd[col].dtype:
            result_polars_pd[col] = result_polars_pd[col].astype(str)
            result_spark_pd[col] = result_spark_pd[col].astype(str)

# 4. VÃ‰RIFICATION D'Ã‰GALITÃ‰ STRICTE
print("\nâ–º TEST D'Ã‰GALITÃ‰ STRICTE:")

# Test 1: Ã‰galitÃ© complÃ¨te
if result_polars_pd.equals(result_spark_pd):
    print("âœ“ Les rÃ©sultats sont STRICTEMENT IDENTIQUES !")
    print("  - MÃªme ordre")
    print("  - MÃªmes valeurs")
    print("  - MÃªmes types")
else:
    print("âš  Les rÃ©sultats prÃ©sentent des diffÃ©rences\n")
    
    # Test 2: Ã‰galitÃ© des valeurs (ignorer les types)
    try:
        # Comparer valeur par valeur
        values_equal = (result_polars_pd.values == result_spark_pd.values).all()
        if values_equal:
            print("âœ“ Les VALEURS sont identiques (diffÃ©rence de types seulement)")
        else:
            print("âœ— Les VALEURS diffÃ¨rent")
    except:
        print("âœ— Impossible de comparer les valeurs directement")
    
    # Test 3: VÃ©rification Ã©lÃ©ment par Ã©lÃ©ment
    print("\nâ–º DÃ‰TAILS DES DIFFÃ‰RENCES:")
    
    # Comparer les shapes
    if result_polars_pd.shape != result_spark_pd.shape:
        print(f"âœ— Shapes diffÃ©rentes:")
        print(f"  Polars: {result_polars_pd.shape}")
        print(f"  Spark:  {result_spark_pd.shape}")
    else:
        print(f"âœ“ MÃªme shape: {result_polars_pd.shape}")
    
    # Comparer ligne par ligne
    differences_found = False
    for idx in range(min(len(result_polars_pd), len(result_spark_pd))):
        polars_row = result_polars_pd.iloc[idx]
        spark_row = result_spark_pd.iloc[idx]
        
        if not polars_row.equals(spark_row):
            if not differences_found:
                print("\nâœ— PremiÃ¨res diffÃ©rences trouvÃ©es:")
                differences_found = True
            
            if idx < 5:  # Afficher seulement les 5 premiÃ¨res diffÃ©rences
                print(f"\n  Ligne {idx}:")
                print(f"    Polars: {polars_row.to_dict()}")
                print(f"    Spark:  {spark_row.to_dict()}")
    
    if not differences_found:
        print("âœ“ Toutes les lignes sont identiques (diffÃ©rence de mÃ©tadonnÃ©es seulement)")

print("\n" + "="*60)
# Nettoyage
spark.stop()
print("\nâœ“ Spark arrÃªtÃ©")